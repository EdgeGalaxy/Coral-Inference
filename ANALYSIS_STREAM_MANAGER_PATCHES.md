# Stream Manager Patches è°ƒç”¨é“¾åˆ†æä¸ä¼˜åŒ–æ–¹æ¡ˆ

## æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è®°å½•äº†å¯¹ Coral-Inference é¡¹ç›®ä¸­ stream_manager patches çš„å®Œæ•´åˆ†æï¼ŒåŒ…æ‹¬è°ƒç”¨é“¾åˆ†æã€é—®é¢˜è¯†åˆ«å’Œä¼˜åŒ–å»ºè®®ã€‚

## è°ƒç”¨é“¾åˆ†æ

### 1. å…¥å£ç‚¹åˆ†æ

**ä¸»è¦å…¥å£ï¼š** `/coral_inference/core/__init__.py`

```python
# Line 57-65: Patchåº”ç”¨ç‚¹
app.InferencePipelinesManagerHandler.handle = patch_app.rewrite_handle
app.get_response_ignoring_thrash = patch_app.patched_get_response_ignoring_thrash
app.handle_command = patch_app.patched_handle_command
app.execute_termination = patch_app.patched_execute_termination
app.join_inference_pipeline = patch_app.patched_join_inference_pipeline
app.check_process_health = patch_app.patched_check_process_health
app.ensure_idle_pipelines_warmed_up = patch_app.patched_ensure_idle_pipelines_warmed_up
```

### 2. å®Œæ•´è°ƒç”¨é“¾è·¯

```
ç”¨æˆ·è¯·æ±‚
  â†“
InferencePipelinesManagerHandler.handle (patched)
  â†“
receive_socket_data() â†’ è§£æå‘½ä»¤
  â†“
patched_handle_command() â†’ å¤„ç†å‘½ä»¤
  â†“
safe_queue_put() â†’ é˜Ÿåˆ—æ“ä½œ
  â†“
patched_get_response_ignoring_thrash() â†’ è·å–å“åº”
  â†“
send_data_trough_socket() â†’ è¿”å›ç»“æœ
```

**å¥åº·æ£€æŸ¥å¹¶è¡Œé“¾è·¯ï¼š**
```
patched_check_process_health() (åå°daemon)
  â†“
perform_safe_health_check() â†’ æ¯ä¸ªpipeline
  â†“
patched_handle_command() â†’ STATUSå‘½ä»¤
  â†“
terminate_pipeline_async() â†’ å¦‚æœå¤±è´¥
```

**è¿›ç¨‹ç»ˆæ­¢é“¾è·¯ï¼š**
```
Signal Handler â†’ patched_execute_termination()
  â†“
Phase 1: æ ‡è®°æ‰€æœ‰pipelineä¸ºremoval
  â†“
Phase 2: å‘é€terminateä¿¡å· + ç­‰å¾…
  â†“
Phase 3: å¼ºåˆ¶kill + cleanup
```

## å½“å‰å®ç°é—®é¢˜åˆ†æ

### 1. æ¶æ„å±‚é¢é—®é¢˜

#### é—®é¢˜1: é‡å¤å‡½æ•°å®šä¹‰
**ä½ç½®ï¼š** `patch_app.py` line 219-281 vs line 638-653
```python
# ä¸¤ä¸ªä¸åŒçš„terminateå®ç°
def patched_execute_termination()  # è¢«ä½¿ç”¨çš„ç‰ˆæœ¬
def rewrite_execute_termination()  # æœªä½¿ç”¨ä½†å­˜åœ¨çš„ç‰ˆæœ¬
```

**å½±å“ï¼š** ä»£ç æ··æ·†ï¼Œç»´æŠ¤å›°éš¾

#### é—®é¢˜2: å…¨å±€çŠ¶æ€ç®¡ç†é£é™©
**ä½ç½®ï¼š** `patch_app.py` line 82-85
```python
# å…¨å±€å˜é‡å­˜åœ¨ç«æ€æ¡ä»¶é£é™©
SHUTDOWN_EVENT = Event()
PIPELINE_HEALTH = {}  # æ— é”ä¿æŠ¤çš„å…¨å±€å­—å…¸
```

**å½±å“ï¼š** å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯èƒ½å¯¼è‡´çŠ¶æ€ä¸ä¸€è‡´

#### é—®é¢˜3: Monkey Patchingçš„ç»´æŠ¤æ€§é—®é¢˜
**ä½ç½®ï¼š** `__init__.py` line 57-65
- ç›´æ¥æ›¿æ¢åŸå‡½æ•°ï¼Œè°ƒè¯•å›°éš¾
- ç‰ˆæœ¬å‡çº§æ—¶å®¹æ˜“å‡ºç°å…¼å®¹æ€§é—®é¢˜
- å‡½æ•°ç­¾åå˜åŒ–æ—¶éœ€è¦åŒæ­¥ä¿®æ”¹

### 2. å®ç°ç»†èŠ‚é—®é¢˜

#### é—®é¢˜4: è¶…æ—¶ç­–ç•¥è¿‡äºé™æ€
**ä½ç½®ï¼š** `patch_app.py` line 75-79
```python
# å›ºå®šè¶…æ—¶æ—¶é—´ï¼Œæ— æ³•é€‚åº”ä¸åŒè´Ÿè½½
QUEUE_TIMEOUT = float(os.getenv("STREAM_MANAGER_QUEUE_TIMEOUT", "10.0"))
HEALTH_CHECK_TIMEOUT = float(os.getenv("STREAM_MANAGER_HEALTH_CHECK_TIMEOUT", "5.0"))
```

**å½±å“ï¼š**
- é«˜è´Ÿè½½æ—¶å¯èƒ½è¶…æ—¶è¿‡å¿«
- ä½è´Ÿè½½æ—¶ç­‰å¾…æ—¶é—´è¿‡é•¿

#### é—®é¢˜5: çº¿ç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸è¶³
**ä½ç½®ï¼š** `patch_app.py` line 102-105
```python
# daemonçº¿ç¨‹å¯èƒ½åœ¨ä¸»è¿›ç¨‹é€€å‡ºæ—¶ä¸¢å¤±
thread = threading.Thread(target=target)
thread.daemon = True
thread.start()
```

**å½±å“ï¼š** å¯èƒ½å¯¼è‡´èµ„æºæ³„éœ²æˆ–ä¸å®Œæ•´çš„æ¸…ç†

## æ€§èƒ½åˆ†æ

### 1. å½“å‰æ€§èƒ½ç‰¹ç‚¹

**ä¼˜ç‚¹ï¼š**
- âœ… è¶…æ—¶ä¿æŠ¤æœ‰æ•ˆé˜²æ­¢æ­»é”
- âœ… é˜Ÿåˆ—æ“ä½œå®‰å…¨åŒ…è£…
- âœ… å¥åº·æ£€æŸ¥æœºåˆ¶å®Œå–„
- âœ… ä¼˜é›…çš„è¿›ç¨‹ç»ˆæ­¢

**æ€§èƒ½å¼€é”€ï¼š**
- ğŸ”¶ æ¯ä¸ªå‘½ä»¤éƒ½æœ‰è¶…æ—¶çº¿ç¨‹å¼€é”€
- ğŸ”¶ å¥åº·æ£€æŸ¥daemonæŒç»­è¿è¡Œ
- ğŸ”¶ é˜Ÿåˆ—æ“ä½œé¢å¤–çš„try-catchåŒ…è£…

### 2. ç“¶é¢ˆè¯†åˆ«

1. **çº¿ç¨‹åˆ›å»ºå¼€é”€**ï¼šæ¯ä¸ªè¶…æ—¶æ“ä½œéƒ½åˆ›å»ºæ–°çº¿ç¨‹
2. **å…¨å±€é”ç«äº‰**ï¼šPROCESSES_TABLE_LOCKå¯èƒ½æˆä¸ºç“¶é¢ˆ
3. **é‡å¤å¥åº·æ£€æŸ¥**ï¼šæ‰€æœ‰pipelineä½¿ç”¨ç›¸åŒæ£€æŸ¥é¢‘ç‡

## ä¼˜åŒ–æ–¹æ¡ˆ

### 1. ç«‹å³ä¿®å¤æ–¹æ¡ˆ

#### ä¿®å¤1: æ¸…ç†é‡å¤å‡½æ•°
```python
# åœ¨ __init__.py ä¸­ç§»é™¤æœªä½¿ç”¨çš„å‡½æ•°å¼•ç”¨
# åªä¿ç•™ä¸€å¥—ä¸€è‡´çš„patchå‡½æ•°
app.execute_termination = patch_app.patched_execute_termination
# åˆ é™¤ rewrite_execute_termination ç›¸å…³ä»£ç 
```

#### ä¿®å¤2: æ”¹è¿›å…¨å±€çŠ¶æ€ç®¡ç†
```python
class PipelineStateManager:
    """çº¿ç¨‹å®‰å…¨çš„pipelineçŠ¶æ€ç®¡ç†å™¨"""
    def __init__(self):
        self.pipeline_health = {}
        self.shutdown_event = Event()
        self._lock = threading.RLock()

    def get_pipeline_health(self, pipeline_id: str) -> dict:
        with self._lock:
            return self.pipeline_health.get(pipeline_id, {
                'failures': 0,
                'last_check': time.time(),
                'marked_for_removal': False
            })

    def update_pipeline_health(self, pipeline_id: str, health_data: dict):
        with self._lock:
            if pipeline_id not in self.pipeline_health:
                self.pipeline_health[pipeline_id] = {}
            self.pipeline_health[pipeline_id].update(health_data)

    def mark_for_removal(self, pipeline_id: str):
        with self._lock:
            if pipeline_id in self.pipeline_health:
                self.pipeline_health[pipeline_id]['marked_for_removal'] = True

    def cleanup_pipeline(self, pipeline_id: str):
        with self._lock:
            self.pipeline_health.pop(pipeline_id, None)

# å…¨å±€å®ä¾‹
PIPELINE_STATE = PipelineStateManager()
```

#### ä¿®å¤3: é…ç½®é›†ä¸­åŒ–
```python
class StreamManagerConfig:
    """é›†ä¸­é…ç½®ç®¡ç†"""
    def __init__(self):
        self.QUEUE_TIMEOUT = float(os.getenv("STREAM_MANAGER_QUEUE_TIMEOUT", "10.0"))
        self.HEALTH_CHECK_TIMEOUT = float(os.getenv("STREAM_MANAGER_HEALTH_CHECK_TIMEOUT", "5.0"))
        self.MAX_HEALTH_FAILURES = int(os.getenv("STREAM_MANAGER_MAX_HEALTH_FAILURES", "3"))
        self.PROCESS_JOIN_TIMEOUT = float(os.getenv("STREAM_MANAGER_PROCESS_JOIN_TIMEOUT", "30.0"))
        self.TERMINATION_GRACE_PERIOD = float(os.getenv("STREAM_MANAGER_TERMINATION_GRACE_PERIOD", "5.0"))

        self.validate()

    def validate(self):
        """éªŒè¯é…ç½®å‚æ•°åˆç†æ€§"""
        if self.QUEUE_TIMEOUT <= 0:
            raise ValueError("QUEUE_TIMEOUT must be positive")
        if self.HEALTH_CHECK_TIMEOUT <= 0:
            raise ValueError("HEALTH_CHECK_TIMEOUT must be positive")
        if self.MAX_HEALTH_FAILURES < 1:
            raise ValueError("MAX_HEALTH_FAILURES must be at least 1")

CONFIG = StreamManagerConfig()
```

### 2. ä¸­æœŸé‡æ„æ–¹æ¡ˆ

#### é‡æ„1: è£…é¥°å™¨æ¨¡å¼æ›¿ä»£Monkey Patching
```python
class StreamManagerPatches:
    """ä½¿ç”¨è£…é¥°å™¨æ¨¡å¼çš„patchç®¡ç†å™¨"""
    def __init__(self, config: StreamManagerConfig):
        self.config = config
        self.state = PipelineStateManager()
        self.original_functions = {}

    def apply_patches(self):
        """é›†ä¸­åº”ç”¨æ‰€æœ‰patch"""
        self._patch_function('handle_command', app.handle_command, self._handle_command_wrapper)
        self._patch_function('get_response_ignoring_thrash', app.get_response_ignoring_thrash, self._get_response_wrapper)
        # ... å…¶ä»–patch

    def _patch_function(self, name: str, original_func, wrapper_func):
        """å®‰å…¨åœ°patchå‡½æ•°"""
        self.original_functions[name] = original_func
        setattr(app, name, wrapper_func)

    def restore_patches(self):
        """æ¢å¤åŸå§‹å‡½æ•°ï¼ˆç”¨äºæµ‹è¯•æˆ–æ¸…ç†ï¼‰"""
        for name, original_func in self.original_functions.items():
            setattr(app, name, original_func)

    def _handle_command_wrapper(self, *args, **kwargs):
        """handle_commandçš„åŒ…è£…å™¨"""
        return self._execute_with_timeout(
            self.original_functions['handle_command'],
            args, kwargs,
            timeout=self.config.QUEUE_TIMEOUT
        )

# ä½¿ç”¨æ–¹å¼
patches = StreamManagerPatches(CONFIG)
patches.apply_patches()
```

#### é‡æ„2: åŠ¨æ€è¶…æ—¶ç®¡ç†
```python
class AdaptiveTimeoutManager:
    """è‡ªé€‚åº”è¶…æ—¶ç®¡ç†å™¨"""
    def __init__(self, base_config: StreamManagerConfig):
        self.base_config = base_config
        self.operation_history = defaultdict(lambda: deque(maxlen=100))
        self.timeout_multipliers = {
            'health_check': 1.0,
            'command_execution': 1.0,
            'queue_operation': 1.0,
        }

    def record_operation(self, operation_type: str, duration: float, success: bool):
        """è®°å½•æ“ä½œå†å²"""
        self.operation_history[operation_type].append({
            'duration': duration,
            'success': success,
            'timestamp': time.time()
        })
        self._update_timeout_multiplier(operation_type)

    def get_timeout(self, operation_type: str) -> float:
        """è·å–åŠ¨æ€è°ƒæ•´çš„è¶…æ—¶æ—¶é—´"""
        base_timeout = getattr(self.base_config, f"{operation_type.upper()}_TIMEOUT", 10.0)
        multiplier = self.timeout_multipliers.get(operation_type, 1.0)
        return base_timeout * multiplier

    def _update_timeout_multiplier(self, operation_type: str):
        """æ ¹æ®å†å²æ•°æ®æ›´æ–°è¶…æ—¶å€æ•°"""
        history = self.operation_history[operation_type]
        if len(history) < 10:
            return

        recent_failures = sum(1 for record in list(history)[-10:] if not record['success'])
        failure_rate = recent_failures / 10

        if failure_rate > 0.3:  # 30%å¤±è´¥ç‡
            self.timeout_multipliers[operation_type] = min(2.0, self.timeout_multipliers[operation_type] * 1.1)
        elif failure_rate < 0.1:  # 10%å¤±è´¥ç‡
            self.timeout_multipliers[operation_type] = max(0.5, self.timeout_multipliers[operation_type] * 0.95)

timeout_manager = AdaptiveTimeoutManager(CONFIG)
```

#### é‡æ„3: æ™ºèƒ½å¥åº·æ£€æŸ¥
```python
class IntelligentHealthChecker:
    """æ™ºèƒ½å¥åº·æ£€æŸ¥å™¨"""
    def __init__(self, config: StreamManagerConfig, state: PipelineStateManager):
        self.config = config
        self.state = state
        self.check_intervals = {}
        self.base_interval = 10.0

    def get_check_interval(self, pipeline_id: str) -> float:
        """æ ¹æ®pipelineçŠ¶å†µåŠ¨æ€è°ƒæ•´æ£€æŸ¥é¢‘ç‡"""
        health = self.state.get_pipeline_health(pipeline_id)
        failure_count = health.get('failures', 0)

        if failure_count == 0:
            return self.base_interval * 2  # å¥åº·pipelineé™ä½é¢‘ç‡
        elif failure_count < self.config.MAX_HEALTH_FAILURES // 2:
            return self.base_interval
        else:
            return self.base_interval / 2  # ä¸å¥åº·pipelineå¢åŠ é¢‘ç‡

    def should_check_now(self, pipeline_id: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç«‹å³æ£€æŸ¥"""
        health = self.state.get_pipeline_health(pipeline_id)
        last_check = health.get('last_check', 0)
        interval = self.get_check_interval(pipeline_id)

        return time.time() - last_check >= interval

    async def perform_health_check(self, pipeline_id: str, managed_pipeline) -> bool:
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        start_time = time.time()
        try:
            # ä½¿ç”¨åŠ¨æ€è¶…æ—¶
            timeout = timeout_manager.get_timeout('health_check')
            result = await self._execute_health_check_with_timeout(
                pipeline_id, managed_pipeline, timeout
            )

            duration = time.time() - start_time
            timeout_manager.record_operation('health_check', duration, result)

            return result
        except Exception as e:
            duration = time.time() - start_time
            timeout_manager.record_operation('health_check', duration, False)
            logger.warning(f"Health check failed for {pipeline_id}: {e}")
            return False

health_checker = IntelligentHealthChecker(CONFIG, PIPELINE_STATE)
```

### 3. é•¿æœŸæ¶æ„æ”¹è¿›

#### æ”¹è¿›1: æ’ä»¶åŒ–æ¶æ„
```python
class PatchPlugin:
    """Patchæ’ä»¶åŸºç±»"""
    def __init__(self, name: str):
        self.name = name

    def apply(self, target_module):
        """åº”ç”¨patch"""
        raise NotImplementedError

    def remove(self, target_module):
        """ç§»é™¤patch"""
        raise NotImplementedError

class TimeoutPlugin(PatchPlugin):
    """è¶…æ—¶ä¿æŠ¤æ’ä»¶"""
    def apply(self, target_module):
        # å®ç°è¶…æ—¶ä¿æŠ¤é€»è¾‘
        pass

class HealthCheckPlugin(PatchPlugin):
    """å¥åº·æ£€æŸ¥æ’ä»¶"""
    def apply(self, target_module):
        # å®ç°å¥åº·æ£€æŸ¥é€»è¾‘
        pass

class PatchManager:
    """æ’ä»¶ç®¡ç†å™¨"""
    def __init__(self):
        self.plugins = []

    def register_plugin(self, plugin: PatchPlugin):
        self.plugins.append(plugin)

    def apply_all(self, target_module):
        for plugin in self.plugins:
            plugin.apply(target_module)
```

#### æ”¹è¿›2: å¼‚æ­¥æ¶æ„æ”¯æŒ
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncStreamManager:
    """å¼‚æ­¥æµç®¡ç†å™¨"""
    def __init__(self, config: StreamManagerConfig):
        self.config = config
        self.executor = ThreadPoolExecutor(max_workers=config.MAX_CONCURRENT_OPERATIONS)

    async def handle_command_async(self, processes_table, request_id, pipeline_id, command):
        """å¼‚æ­¥å‘½ä»¤å¤„ç†"""
        loop = asyncio.get_event_loop()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
        result = await loop.run_in_executor(
            self.executor,
            self._handle_command_sync,
            processes_table, request_id, pipeline_id, command
        )
        return result

    async def health_check_daemon_async(self):
        """å¼‚æ­¥å¥åº·æ£€æŸ¥daemon"""
        while not PIPELINE_STATE.shutdown_event.is_set():
            tasks = []

            # åˆ›å»ºå¹¶å‘å¥åº·æ£€æŸ¥ä»»åŠ¡
            for pipeline_id, managed_pipeline in app.PROCESSES_TABLE.items():
                if health_checker.should_check_now(pipeline_id):
                    task = health_checker.perform_health_check(pipeline_id, managed_pipeline)
                    tasks.append(task)

            if tasks:
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥
                results = await asyncio.gather(*tasks, return_exceptions=True)
                # å¤„ç†ç»“æœ...

            await asyncio.sleep(1.0)  # ç­‰å¾…1ç§’å†æ¬¡æ£€æŸ¥
```

## ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### 1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†
```python
class StreamManagerMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    def __init__(self):
        self.metrics = {
            'command_execution_time': deque(maxlen=1000),
            'health_check_duration': deque(maxlen=1000),
            'queue_operation_time': deque(maxlen=1000),
            'pipeline_failures': Counter(),
            'timeout_events': Counter(),
            'memory_usage': deque(maxlen=100),
        }

    def record_command_execution(self, duration: float, success: bool):
        self.metrics['command_execution_time'].append({
            'duration': duration,
            'success': success,
            'timestamp': time.time()
        })

    def get_performance_summary(self) -> dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        recent_commands = list(self.metrics['command_execution_time'])[-100:]

        if not recent_commands:
            return {}

        durations = [cmd['duration'] for cmd in recent_commands]
        success_rate = sum(1 for cmd in recent_commands if cmd['success']) / len(recent_commands)

        return {
            'avg_command_time': statistics.mean(durations),
            'p95_command_time': statistics.quantiles(durations, n=20)[18] if len(durations) > 20 else max(durations),
            'success_rate': success_rate,
            'total_timeouts': sum(self.metrics['timeout_events'].values()),
            'active_pipelines': len([pid for pid in app.PROCESSES_TABLE.keys()
                                   if not PIPELINE_STATE.get_pipeline_health(pid).get('marked_for_removal', False)]),
        }

metrics = StreamManagerMetrics()
```

### 2. æ—¥å¿—å¢å¼º
```python
class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    def __init__(self):
        self.logger = logger

    def log_command_execution(self, pipeline_id: str, command_type: str,
                             duration: float, success: bool, error: str = None):
        """è®°å½•å‘½ä»¤æ‰§è¡Œæ—¥å¿—"""
        log_data = {
            'event': 'command_execution',
            'pipeline_id': pipeline_id,
            'command_type': command_type,
            'duration_ms': duration * 1000,
            'success': success,
            'timestamp': time.time(),
        }

        if error:
            log_data['error'] = str(error)

        if success:
            self.logger.info("Command executed successfully", extra=log_data)
        else:
            self.logger.error("Command execution failed", extra=log_data)

    def log_health_check(self, pipeline_id: str, duration: float,
                        success: bool, failure_count: int):
        """è®°å½•å¥åº·æ£€æŸ¥æ—¥å¿—"""
        log_data = {
            'event': 'health_check',
            'pipeline_id': pipeline_id,
            'duration_ms': duration * 1000,
            'success': success,
            'failure_count': failure_count,
            'timestamp': time.time(),
        }

        if success:
            self.logger.debug("Health check passed", extra=log_data)
        else:
            self.logger.warning("Health check failed", extra=log_data)

structured_logger = StructuredLogger()
```

## å®æ–½å»ºè®®

### 1. åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

**Phase 1: ç«‹å³ä¿®å¤ï¼ˆ1-2å¤©ï¼‰**
- [ ] æ¸…ç†é‡å¤å‡½æ•°å®šä¹‰
- [ ] æ·»åŠ é…ç½®éªŒè¯
- [ ] æ”¹è¿›æ—¥å¿—è®°å½•

**Phase 2: çŠ¶æ€ç®¡ç†é‡æ„ï¼ˆ1å‘¨ï¼‰**
- [ ] å®ç°PipelineStateManager
- [ ] æ”¹è¿›å…¨å±€çŠ¶æ€ç®¡ç†
- [ ] æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ”¶é›†

**Phase 3: æ¶æ„ä¼˜åŒ–ï¼ˆ2-3å‘¨ï¼‰**
- [ ] å®ç°è£…é¥°å™¨æ¨¡å¼
- [ ] åŠ¨æ€è¶…æ—¶ç®¡ç†
- [ ] æ™ºèƒ½å¥åº·æ£€æŸ¥

**Phase 4: é•¿æœŸæ”¹è¿›ï¼ˆ1-2æœˆï¼‰**
- [ ] æ’ä»¶åŒ–æ¶æ„
- [ ] å¼‚æ­¥æ¶æ„æ”¯æŒ
- [ ] åˆ†å¸ƒå¼éƒ¨ç½²æ”¯æŒ

### 2. æµ‹è¯•ç­–ç•¥

**å•å…ƒæµ‹è¯•ï¼š**
```python
def test_pipeline_state_manager():
    state = PipelineStateManager()

    # æµ‹è¯•åŸºæœ¬æ“ä½œ
    state.update_pipeline_health('test-1', {'failures': 0})
    health = state.get_pipeline_health('test-1')
    assert health['failures'] == 0

    # æµ‹è¯•çº¿ç¨‹å®‰å…¨
    import threading
    def update_health():
        for i in range(100):
            state.update_pipeline_health('test-1', {'failures': i})

    threads = [threading.Thread(target=update_health) for _ in range(10)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()

    # éªŒè¯æœ€ç»ˆçŠ¶æ€ä¸€è‡´æ€§
    final_health = state.get_pipeline_health('test-1')
    assert isinstance(final_health['failures'], int)
```

**é›†æˆæµ‹è¯•ï¼š**
```python
def test_adaptive_timeout_integration():
    """æµ‹è¯•è‡ªé€‚åº”è¶…æ—¶åœ¨çœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°"""
    timeout_manager = AdaptiveTimeoutManager(CONFIG)

    # æ¨¡æ‹Ÿå¤šæ¬¡æ“ä½œ
    for i in range(50):
        start_time = time.time()
        # æ‰§è¡Œæ¨¡æ‹Ÿæ“ä½œ
        success = simulate_operation()
        duration = time.time() - start_time

        timeout_manager.record_operation('health_check', duration, success)

    # éªŒè¯è¶…æ—¶æ—¶é—´å·²è°ƒæ•´
    initial_timeout = CONFIG.HEALTH_CHECK_TIMEOUT
    adapted_timeout = timeout_manager.get_timeout('health_check')

    assert adapted_timeout != initial_timeout  # åº”è¯¥æœ‰è°ƒæ•´
```

**å‹åŠ›æµ‹è¯•ï¼š**
```python
async def test_concurrent_pipeline_operations():
    """æµ‹è¯•å¹¶å‘pipelineæ“ä½œçš„ç¨³å®šæ€§"""
    async def create_and_destroy_pipeline():
        pipeline_id = f"test-{uuid.uuid4().hex}"
        # åˆ›å»ºpipeline
        # æ‰§è¡Œæ“ä½œ
        # é”€æ¯pipeline
        return pipeline_id

    # å¹¶å‘æ‰§è¡Œ100ä¸ªpipelineæ“ä½œ
    tasks = [create_and_destroy_pipeline() for _ in range(100)]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # éªŒè¯æ²¡æœ‰èµ„æºæ³„éœ²
    assert len(app.PROCESSES_TABLE) == 0
    assert len(PIPELINE_STATE.pipeline_health) == 0
```

## æ€»ç»“

å½“å‰çš„patchæ–¹æ¡ˆæœ‰æ•ˆè§£å†³äº†stream managerçš„è¶…æ—¶å’Œæ­»é”é—®é¢˜ï¼Œä½†åœ¨ä»£ç ç»“æ„å’Œå¯ç»´æŠ¤æ€§æ–¹é¢å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚å»ºè®®æŒ‰ç…§åˆ†é˜¶æ®µçš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ï¼Œé¦–å…ˆè§£å†³ç«‹å³çš„é—®é¢˜ï¼Œç„¶åé€æ­¥è¿›è¡Œæ¶æ„æ”¹è¿›ã€‚

å…³é”®æ”¹è¿›ç‚¹ï¼š
1. **ä»£ç æ¸…æ´åº¦**ï¼šæ¶ˆé™¤é‡å¤å®šä¹‰ï¼Œæ”¹è¿›å‘½åä¸€è‡´æ€§
2. **çŠ¶æ€ç®¡ç†**ï¼šä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†å™¨
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šåŠ¨æ€è¶…æ—¶ã€æ™ºèƒ½å¥åº·æ£€æŸ¥
4. **å¯è§‚æµ‹æ€§**ï¼šå¢å¼ºç›‘æ§å’Œæ—¥å¿—è®°å½•
5. **æ¶æ„æ¼”è¿›**ï¼šå‘æ’ä»¶åŒ–ã€å¼‚æ­¥åŒ–æ–¹å‘å‘å±•

è¿™äº›æ”¹è¿›å°†æ˜¾è‘—æå‡ç³»ç»Ÿçš„ç¨³å®šæ€§ã€æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚